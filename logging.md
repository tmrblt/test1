
## Installing the logging subsystem for Red Hat OpenShift

**Logging architecture**

> **Collector** - Reads container log data from each node and forwards log data to configured outputs.

> **Store** - Stores log data for analysis; the default output for the forwarder.

> **Visualization** - Graphical interface for searching, querying, and viewing stored logs.

**Log categories**

> **application** - Container logs generated by user applications running in the cluster, except infrastructure container applications.

> **infrastructure** - Logs generated by nfrastructure components running in the cluster and OpenShift Container Platform odes, such as journal logs. nfrastructure components are pods that run in the openshift*, kube*, or default projects.

> **audit** - Logs generated by auditd, the ode audit system, which are stored in he /var/log/audit/audit.log file, and ogs from the auditd, kube-apiserver, penshift-apiserver projects, as well as he ovn project if enabled.

**logging subsystem components**

> **collection** - This is the component that collects logs from the cluster, formats them, and forwards them to the log store. The current implementation is Fluentd.

> **log store** - This is where the logs are stored. The default implementation is Elasticsearch. You can use the default Elasticsearch log store or forward logs to external log stores. The default log store is optimized and tested for short-term storage.

> **visualization** - This is the UI component you can use to view logs, graphs, charts, and so forth. The current implementation is Kibana.

**Install ElasticSearch and Logging Operators**
> You can install the logging subsystem for Red Hat OpenShift by deploying the OpenShift Elasticsearch and Red Hat OpenShift Logging Operators. The OpenShift Elasticsearch Operator creates and manages the Elasticsearch cluster used by OpenShift Logging. The logging subsystem Operator creates and manages the components of the logging stack.

**Create logging instance** \
Administration > CRDs > ClusterLogging > View Instances > Create ClusterLogging > refresh the page > In the YAML field, replace the code with files/logging.yaml

**Verify the installation** \
Workloads > Pods > openshift-logging

**Allow-traffic** \
Label namespace
```
oc label namespace openshift-operators-redhat project=openshift-operators-redhat
```
Create network-policy
```
cat << EOF > np-logging.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-monitoring-ingress-operators-redhat
spec:
  ingress:
  - from:
    - podSelector: {}
  - from:
    - namespaceSelector:
        matchLabels:
          project: "openshift-operators-redhat"
  - from:
    - namespaceSelector:
        matchLabels:
          name: "openshift-monitoring"
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
  podSelector: {}
  policyTypes:
  - Ingress
EOF
```
Apply policy
```
oc apply -f np-logging.yaml
```
**enable audit logging** \
create CR for audit logs
```
cat << EOF > cluster-log-forwarder.yaml
apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: instance
  namespace: openshift-logging
spec:
  pipelines: 
  - name: all-to-default
    inputRefs:
    - infrastructure
    - application
    - audit
    outputRefs:
    - default
EOF
```
```
oc apply -f cluster-log-forwarder.yaml -n openshift-logging
```
**deploy a sample application to test app-logging** \
```
oc new-project test
```
```
oc new-app https://github.com/sclorg/cakephp-ex
```
**enable event logging** \
process the template
```
oc process -f event-router-template.yaml | oc apply -n openshift-logging -f -
```
verify installation
```
oc get pods --selector  component=eventrouter -o name -n openshift-logging
```
**define kibana index-patterns** \
Application Launcher > Logging > Management > Index Patterns > Create index pattern > app, infra, and audit using the @timestamp time fields.

**kibana visualization**
https://www.elastic.co/guide/en/kibana/6.8/connect-to-elasticsearch.html
